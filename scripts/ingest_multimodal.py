import os
import json
import base64
import requests
import re
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from openai import OpenAI
from supabase import create_client, Client
from dotenv import load_dotenv

# OCR æ”¯æŒï¼ˆå¯é€‰ï¼Œç”¨äºå›¾ç‰‡æ–‡å­—æå–ï¼‰
try:
    from PIL import Image
    import pytesseract
    OCR_AVAILABLE = True
except ImportError:
    OCR_AVAILABLE = False
    print("âš ï¸ OCR åŠŸèƒ½æœªå®‰è£…ï¼Œå›¾ç‰‡å¤„ç†å°†ä½¿ç”¨å¤‡é€‰æ–¹æ¡ˆ")

# Playwright æ”¯æŒï¼ˆå¯é€‰ï¼Œç”¨äºæµè§ˆå™¨è‡ªåŠ¨åŒ–æŠ“å–ï¼‰
try:
    from playwright.sync_api import sync_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("ğŸ’¡ Playwright æœªå®‰è£…ï¼Œå¾®ä¿¡å…¬ä¼—å·é“¾æ¥å¯èƒ½æ— æ³•æŠ“å–éœ€è¦éªŒè¯çš„å†…å®¹")
    print("   å®‰è£…å‘½ä»¤: pip install playwright && playwright install chromium")

# 1. åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½ .env æ–‡ä»¶ï¼‰
import pathlib
env_path = pathlib.Path(__file__).parent.parent / '.env'
load_dotenv(dotenv_path=env_path)

# 2. åˆå§‹åŒ–å®¢æˆ·ç«¯
try:
    openai_client = OpenAI(
        api_key=os.getenv("deepseek_API_KEY"),
        base_url="https://api.deepseek.com"
    )
    
    url: str = os.getenv("SUPABASE_URL")
    key: str = os.getenv("SUPABASE_KEY")
    supabase: Client = create_client(url, key)
except Exception as e:
    print(f"âŒ åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ .env æ–‡ä»¶é…ç½®: {e}")
    exit(1)

# 3. æ ¸å¿ƒ Prompt
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ ¡å›­ä¿¡æ¯ç»“æ„åŒ–åŠ©æ‰‹ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ä»è¾“å…¥çš„å†…å®¹ä¸­æå–å…³é”®ä¿¡æ¯ï¼Œå¹¶ä¸¥æ ¼è¾“å‡ºä¸ºä»¥ä¸‹ JSON æ ¼å¼ï¼š

{
    "title": "æ´»åŠ¨æˆ–æ‹›è˜æ ‡é¢˜ï¼ˆå¿…é¡»åŒ…å«å…¬å¸åç§°å’Œå²—ä½åç§°ï¼Œå¦‚ï¼šåº¦å°æ»¡-ç»„ç»‡å‘å±•å²—ä¸AIäº§å“ç»ç†å²—ï¼‰",
    "type": "recruit" (å¦‚æœæ˜¯æ‹›è˜/å®ä¹ ) æˆ– "activity" (å¦‚æœæ˜¯è®²åº§/æ¯”èµ›/æ´»åŠ¨) æˆ– "lecture" (å¦‚æœæ˜¯è®²åº§),
    "source_group": "ä¿¡æ¯å‘å¸ƒæ¥æº (æ³¨æ„ï¼šä¸æ˜¯å…¬å¸åï¼åªèƒ½å¡«ä»¥ä¸‹ä¹‹ä¸€ï¼šCDC, å­¦é™¢å®˜æ–¹, å†…æ¨, æ ¡å‹æ¨è, å…¬å¸å®˜æ–¹, å…¶ä»–)",
    "key_info": {
        "date": "æ´»åŠ¨æ—¥æœŸ (æ ¼å¼å¦‚ 12æœˆ4æ—¥ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸ºç©ºå­—ç¬¦ä¸²)",
        "time": "å…·ä½“æ—¶é—´ (å¦‚ 14:00-16:00ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸ºç©ºå­—ç¬¦ä¸²)",
        "location": "åœ°ç‚¹ (å¦‚ åŒ—äº¬ã€ä¸Šæµ·ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸ºç©ºå­—ç¬¦ä¸²)",
        "deadline": "æˆªæ­¢æ—¥æœŸå’Œæ—¶é—´ (æ ¼å¼å¦‚ 2025å¹´12æœˆ5æ—¥ä¸­åˆ12:00 æˆ– 12æœˆ5æ—¥12:00ï¼Œå¿…é¡»ç²¾ç¡®æå–å®Œæ•´çš„æ—¶é—´ä¿¡æ¯ï¼ŒåŒ…æ‹¬æ—¥æœŸå’Œæ—¶é—´éƒ¨åˆ†ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸ºç©ºå­—ç¬¦ä¸²)",
        "company": "å…¬å¸åç§° (å¦‚æœæ˜¯æ‹›è˜/å®ä¹ ï¼Œå¿…é¡»æå–å…¬å¸åç§°ï¼Œå¦‚ï¼šåº¦å°æ»¡ã€ç¾å›¢ç­‰ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸ºç©ºå­—ç¬¦ä¸²)",
        "position": "å²—ä½åç§° (å¯ä»¥æ˜¯å­—ç¬¦ä¸²ï¼Œå¦‚æœæœ‰å¤šä¸ªå²—ä½ç”¨'ä¸'æˆ–'ã€'è¿æ¥ï¼Œå¦‚ï¼šç»„ç»‡å‘å±•å²—ä¸AIäº§å“ç»ç†å²—ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸ºç©ºå­—ç¬¦ä¸²)",
        "education": "å­¦å†è¦æ±‚ (å¦‚ï¼š2026å±Šå…¨æ—¥åˆ¶ç¡•å£«åŠä»¥ä¸Šå­¦å†æ¯•ä¸šç”Ÿï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸ºç©ºå­—ç¬¦ä¸²)",
        "link": "æŠ•é€’é“¾æ¥/é—®å·é“¾æ¥ (å®Œæ•´çš„URLï¼Œå¦‚ï¼šhttps://career.wjx.cn/vm/eCMU7Q0.aspxï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸ºç©ºå­—ç¬¦ä¸²)",
        "referral": "æ˜¯å¦å†…æ¨ (true æˆ– falseï¼Œæ ¹æ®å†…å®¹ä¸­æ˜¯å¦åŒ…å«'å†…æ¨'ç­‰ä¿¡æ¯åˆ¤æ–­)"
    },
    "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3"],
    "summary": "ä¸€å¥è¯æ‘˜è¦ (50å­—ä»¥å†…ï¼Œå¿…é¡»åŒ…å«ï¼šå…¬å¸åç§°ã€å²—ä½ç±»å‹ã€å…³é”®è¦æ±‚)",
    "is_valid": true (å¦‚æœæ˜¯æ— å…³é—²èŠï¼Œè®¾ä¸º false)
}

é‡è¦æå–è§„åˆ™ï¼š
1. **å…¬å¸åç§°**ï¼šå¿…é¡»ä»æ ‡é¢˜æˆ–æ­£æ–‡ä¸­æå–å…¬å¸åç§°ï¼Œè¿™æ˜¯æœ€é‡è¦çš„æ ‡è¯†ä¿¡æ¯
2. **å²—ä½åç§°**ï¼šå¦‚æœæœ‰å¤šä¸ªå²—ä½ï¼Œå¿…é¡»å…¨éƒ¨æå–ï¼Œç”¨"ä¸"æˆ–"ã€"è¿æ¥
3. **å­¦å†è¦æ±‚**ï¼šå¿…é¡»æå–æ˜ç¡®çš„å­¦å†è¦æ±‚ï¼ˆå¦‚ï¼š2026å±Šã€ç¡•å£«åŠä»¥ä¸Šç­‰ï¼‰
4. **é“¾æ¥ä¿¡æ¯**ï¼šå¿…é¡»æå–æ‰€æœ‰URLé“¾æ¥ï¼ˆé—®å·é“¾æ¥ã€æŠ•é€’é“¾æ¥ç­‰ï¼‰
5. **å†…æ¨æ ‡è¯†**ï¼šå¦‚æœå†…å®¹ä¸­åŒ…å«"å†…æ¨"ã€"å†…æ¨ç¾¤"ç­‰å…³é”®è¯ï¼Œreferral è®¾ä¸º true
6. **æ ‡é¢˜ä¼˜åŒ–**ï¼šæ ‡é¢˜åº”è¯¥åŒ…å«"å…¬å¸åç§°-å²—ä½åç§°"æˆ–"å…¬å¸åç§°-æ´»åŠ¨åç§°"çš„æ ¼å¼ï¼Œç¡®ä¿ä¿¡æ¯å®Œæ•´
7. **æ ‡ç­¾ç”Ÿæˆ**ï¼šæ ¹æ®å…¬å¸ã€å²—ä½ç±»å‹ã€åœ°ç‚¹ç­‰ç”Ÿæˆ3-5ä¸ªç›¸å…³æ ‡ç­¾
8. **æ¥æºåˆ¤æ–­ï¼ˆé‡è¦ï¼source_group ä¸æ˜¯å…¬å¸åï¼ï¼‰**ï¼š
   - å¦‚æœæ–‡æœ¬ä¸­æåˆ°"CDC"ã€"èŒä¸šå‘å±•ä¸­å¿ƒ" â†’ CDC
   - å¦‚æœæ–‡æœ¬ä¸­æåˆ°"å†…æ¨"ã€"æ¨è" â†’ å†…æ¨
   - å¦‚æœæ–‡æœ¬ä¸­æåˆ°"æ ¡å‹"ã€"å­¦é•¿å­¦å§" â†’ æ ¡å‹æ¨è
   - å¦‚æœæ–‡æœ¬ä¸­æåˆ°"å­¦é™¢"ã€"å®˜æ–¹" â†’ å­¦é™¢å®˜æ–¹
   - å¦‚æœæ˜¯å…¬å¸è‡ªå·±å‘å¸ƒçš„å®˜æ–¹æ‹›è˜ â†’ å…¬å¸å®˜æ–¹
   - å¦‚æœæ¥æºä¸æ˜ç¡® â†’ å…¶ä»–
   - âš ï¸ æ³¨æ„ï¼šå…¬å¸åç§°ï¼ˆå¦‚"äºšæŠ•è¡Œ"ã€"è…¾è®¯"ï¼‰åº”è¯¥æ”¾åœ¨ key_info.company å­—æ®µï¼Œä¸æ˜¯ source_groupï¼
9. **æ—¶é—´ä¿¡æ¯æå–ï¼ˆé‡è¦ï¼‰**ï¼š
   - deadlineï¼šå¿…é¡»ç²¾ç¡®æå–æˆªæ­¢æ—¥æœŸå’Œæ—¶é—´ï¼Œæ ¼å¼å¦‚"2025å¹´12æœˆ5æ—¥ä¸­åˆ12:00"æˆ–"12æœˆ5æ—¥12:00"
   - dateï¼šå¦‚æœæ˜¯æ´»åŠ¨ï¼Œæå–æ´»åŠ¨æ—¥æœŸï¼ˆæ ¼å¼å¦‚"12æœˆ4æ—¥"æˆ–"2025å¹´12æœˆ4æ—¥"ï¼‰
   - timeï¼šå¦‚æœæ˜¯æ´»åŠ¨ï¼Œæå–å…·ä½“æ—¶é—´ï¼ˆæ ¼å¼å¦‚"14:00-16:00"ï¼‰
   - å¦‚æœæ–‡æ¡£ä¸­æœ‰"æˆªæ­¢æ—¶é—´"ã€"æˆªæ­¢æ—¥æœŸ"ã€"æŠ¥åæˆªæ­¢"ã€"æ´»åŠ¨æ—¶é—´"ã€"æ´»åŠ¨æ—¥æœŸ"ç­‰å…³é”®è¯ï¼Œå¿…é¡»æå–å®Œæ•´çš„æ—¶é—´ä¿¡æ¯
   - ä¸è¦é—æ¼æ—¶é—´éƒ¨åˆ†ï¼ˆå¦‚"ä¸­åˆ12:00"ã€"ä¸‹åˆ3ç‚¹"ç­‰ï¼‰
10. **å†…å®¹è´¨é‡å¤„ç†**ï¼š
   - å¦‚æœè¾“å…¥å†…å®¹åŒ…å«å¤§é‡UIå…ƒç´ ã€æŒ‰é’®æ–‡å­—ã€å¹²æ‰°ä¿¡æ¯ï¼Œè¯·å¿½ç•¥è¿™äº›å¹²æ‰°å†…å®¹
   - ä¸“æ³¨äºæå–å®é™…çš„æ´»åŠ¨/æ‹›è˜ä¿¡æ¯ï¼Œå¿½ç•¥"å¾®ä¿¡æ‰«ä¸€æ‰«"ã€"å…³æ³¨å…¬ä¼—å·"ç­‰æ— å…³æ–‡å­—
   - å¦‚æœå†…å®¹è´¨é‡å¾ˆå·®ï¼ˆæœ‰æ•ˆä¿¡æ¯å°‘äº50å­—ï¼‰ï¼Œè¯·å°½å¯èƒ½ä»æ ‡é¢˜å’Œå°‘é‡æœ‰æ•ˆæ–‡æœ¬ä¸­æå–ä¿¡æ¯

æ³¨æ„ï¼šåªè¾“å‡ºçº¯ JSON å­—ç¬¦ä¸²ï¼Œä¸è¦åŒ…å« Markdown ä»£ç å—ã€‚æ‰€æœ‰å­—æ®µéƒ½å¿…é¡»å­˜åœ¨ï¼Œå¦‚æœæ²¡æœ‰å¯¹åº”ä¿¡æ¯åˆ™ä½¿ç”¨ç©ºå­—ç¬¦ä¸² "" æˆ– falseã€‚
"""

def normalize_title(title):
    """
    æ ‡å‡†åŒ–æ ‡é¢˜ï¼Œç”¨äºå»é‡æ¯”è¾ƒ
    1. å»é™¤æ‹¬å·åŠå…¶å†…å®¹ï¼ˆå¦‚ (baseåŒ—äº¬)ï¼‰
    2. å»é™¤å¤šä½™ç©ºæ ¼
    3. å»é™¤ç‰¹æ®Šå­—ç¬¦å’Œå‰ç¼€ï¼ˆå¦‚"å†…æ¨|"ã€"å†…æ¨-"ç­‰ï¼‰
    4. ç»Ÿä¸€æ ¼å¼
    """
    if not title:
        return ""
    
    import re
    # å»é™¤æ‹¬å·åŠå…¶å†…å®¹ï¼Œå¦‚ (baseåŒ—äº¬)ã€ï¼ˆbaseåŒ—äº¬ï¼‰ç­‰
    normalized = re.sub(r'[\(ï¼ˆ].*?[\)ï¼‰]', '', title)
    # å»é™¤å¸¸è§å‰ç¼€ï¼ˆå†…æ¨ç›¸å…³ï¼‰
    normalized = re.sub(r'^å†…æ¨[|-]?', '', normalized)
    normalized = re.sub(r'^å†…æ¨ç¾¤[|-]?', '', normalized)
    # å»é™¤å¤šä½™ç©ºæ ¼
    normalized = re.sub(r'\s+', '', normalized)
    # å»é™¤å¸¸è§åˆ†éš”ç¬¦
    normalized = normalized.replace('-', '').replace('|', '').replace('ï¼š', '').replace(':', '')
    # å»é™¤å¼•å·
    normalized = normalized.replace('"', '').replace('"', '').replace('"', '').replace('"', '')
    return normalized.strip()

def check_duplicate(title, event_type, source_group=None):
    """
    æ£€æŸ¥æ˜¯å¦å­˜åœ¨é‡å¤æ•°æ®ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–æ ‡é¢˜ï¼‰
    è¿”å›: (is_duplicate, existing_id)
    """
    normalized_title = normalize_title(title)
    
    try:
        # è·å–æœ€è¿‘7å¤©å†…çš„æ‰€æœ‰åŒç±»å‹è®°å½•
        seven_days_ago = (datetime.now() - timedelta(days=7)).isoformat()
        all_recent = supabase.table("events")\
            .select("id, title, type, source_group, created_at")\
            .eq("type", event_type)\
            .eq("status", "active")\
            .gte("created_at", seven_days_ago)\
            .execute()
        
        if not all_recent.data:
            return False, None
        
        # å¯¹æ¯æ¡è®°å½•è¿›è¡Œæ ‡å‡†åŒ–æ¯”è¾ƒ
        for existing in all_recent.data:
            existing_normalized = normalize_title(existing['title'])
            
            # å¦‚æœæ ‡å‡†åŒ–åçš„æ ‡é¢˜ç›¸åŒï¼Œè®¤ä¸ºæ˜¯é‡å¤
            if existing_normalized == normalized_title:
                return True, existing['id']
            
            # é¢å¤–æ£€æŸ¥ï¼šå¦‚æœæ ‡é¢˜ç›¸ä¼¼åº¦å¾ˆé«˜ï¼ˆåŒ…å«å…³ç³»ï¼‰ï¼Œä¹Ÿè®¤ä¸ºæ˜¯é‡å¤
            # ä¾‹å¦‚ï¼š"ç¾å›¢-å•†ä¸šåˆ†æå®ä¹ ç”Ÿ-å•†ä¸šåŒ–æˆ˜ç•¥æ–¹å‘" å’Œ "ç¾å›¢-å•†ä¸šåˆ†æå®ä¹ ç”Ÿ-å•†ä¸šåŒ–æˆ˜ç•¥æ–¹å‘(baseåŒ—äº¬)"
            if normalized_title in existing_normalized or existing_normalized in normalized_title:
                # ç¡®ä¿æ ¸å¿ƒéƒ¨åˆ†ç›¸åŒï¼ˆè‡³å°‘åŒ…å«ä¸»è¦å…³é”®è¯ï¼‰
                if len(normalized_title) > 10 and len(existing_normalized) > 10:
                    # è®¡ç®—å…±åŒå­—ç¬¦æ¯”ä¾‹
                    common_chars = set(normalized_title) & set(existing_normalized)
                    similarity = len(common_chars) / max(len(set(normalized_title)), len(set(existing_normalized)))
                    if similarity > 0.8:  # 80% ç›¸ä¼¼åº¦é˜ˆå€¼
                        return True, existing['id']
        
        return False, None
        
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥é‡å¤æ•°æ®æ—¶å‡ºé”™: {e}")
        return False, None

def _is_wechat_url(url):
    """æ£€æµ‹æ˜¯å¦ä¸ºå¾®ä¿¡å…¬ä¼—å·é“¾æ¥"""
    return 'mp.weixin.qq.com' in url

def _clean_html_content(html):
    """æ¸…ç† HTML å†…å®¹ï¼Œç§»é™¤è„šæœ¬å’Œæ ·å¼"""
    # ç§»é™¤è„šæœ¬
    html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
    # ç§»é™¤æ ·å¼
    html = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.DOTALL | re.IGNORECASE)
    return html

def _extract_wechat_content(html):
    """ä»å¾®ä¿¡å…¬ä¼—å· HTML ä¸­æå–æ­£æ–‡å†…å®¹"""
    soup = BeautifulSoup(html, 'html.parser')
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯éªŒè¯é¡µé¢
    page_text = soup.get_text()
    if 'ç¯å¢ƒå¼‚å¸¸' in page_text or 'å®ŒæˆéªŒè¯åå³å¯ç»§ç»­è®¿é—®' in page_text:
        return None
    
    # å¾®ä¿¡å…¬ä¼—å·æ–‡ç« å†…å®¹é€‰æ‹©å™¨
    content_selectors = [
        '#js_content',
        '.rich_media_content',
        '#activity-name',
        '.rich_media_title'
    ]
    
    article_parts = []
    for selector in content_selectors:
        elements = soup.select(selector)
        for elem in elements:
            text = elem.get_text(separator='\n', strip=True)
            if text and len(text) > 20:  # è¿‡æ»¤å¤ªçŸ­çš„å†…å®¹
                article_parts.append(text)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šé€‰æ‹©å™¨çš„å†…å®¹ï¼Œè¯´æ˜å¯èƒ½æ˜¯éªŒè¯é¡µé¢æˆ–åŠ¨æ€åŠ è½½
    if not article_parts:
        # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„éªŒè¯æç¤º
        body = soup.find('body')
        if body:
            body_text = body.get_text(separator='\n', strip=True)
            # å¦‚æœ body ä¸­åŒ…å«å¤§é‡å¹²æ‰°ä¿¡æ¯ï¼Œå¯èƒ½æ˜¯éªŒè¯é¡µé¢
            noise_keywords = ['å¾®ä¿¡æ‰«ä¸€æ‰«', 'å…³æ³¨è¯¥å…¬ä¼—å·', 'å–æ¶ˆ', 'å…è®¸', 'çŸ¥é“äº†', 'ä½¿ç”¨å°ç¨‹åº']
            noise_count = sum(1 for keyword in noise_keywords if keyword in body_text)
            if noise_count > 5:  # å¹²æ‰°å…³é”®è¯è¿‡å¤šï¼Œå¯èƒ½æ˜¯éªŒè¯é¡µé¢
                return None
            # å¦‚æœå†…å®¹é•¿åº¦è¶³å¤Ÿï¼Œå°è¯•æå–
            if len(body_text) > 100:
                article_parts.append(body_text)
    
    if article_parts:
        content = '\n\n'.join(article_parts)
        
        # æ¸…ç†å¹²æ‰°æ–‡æœ¬ï¼ˆæ›´å…¨é¢çš„æ¨¡å¼ï¼‰
        noise_patterns = [
            r'åœ¨å°è¯´é˜…è¯»å™¨ä¸­æ²‰æµ¸é˜…è¯»',
            r'é¢„è§ˆæ—¶æ ‡ç­¾ä¸å¯ç‚¹',
            r'å¾®ä¿¡æ‰«ä¸€æ‰«[^ï¼Œã€‚]*',
            r'å…³æ³¨è¯¥å…¬ä¼—å·',
            r'ç»§ç»­æ»‘åŠ¨çœ‹ä¸‹ä¸€ä¸ª',
            r'è½»è§¦é˜…è¯»åŸæ–‡',
            r'å‘ä¸Šæ»‘åŠ¨çœ‹ä¸‹ä¸€ä¸ª',
            r'çŸ¥é“äº†',
            r'å–æ¶ˆ\s*å…è®¸',
            r'å…è®¸\s*å–æ¶ˆ',
            r'ä½¿ç”¨å°ç¨‹åº',
            r'åˆ†æ',
            r'ä½¿ç”¨å®Œæ•´æœåŠ¡',
            r'è§†é¢‘',
            r'å°ç¨‹åº',
            r'èµ[^ï¼Œã€‚]*å–æ¶ˆèµ',
            r'åœ¨çœ‹[^ï¼Œã€‚]*å–æ¶ˆåœ¨çœ‹',
            r'åˆ†äº«',
            r'ç•™è¨€',
            r'æ”¶è—',
            r'å¬è¿‡',
            r'Ã—',
            r'ï¼š\s*ï¼Œ',
            r'ï¼Œ\s*ï¼Œ',
            r'^\s*åŸåˆ›\s*$',
            r'^\s*TIANYAN\s*$',
        ]
        for pattern in noise_patterns:
            content = re.sub(pattern, '', content, flags=re.IGNORECASE | re.MULTILINE)
        
        # æ¸…ç†å¤šä½™ç©ºè¡Œå’Œæ ‡ç‚¹
        content = re.sub(r'\n{3,}', '\n\n', content)
        content = re.sub(r'[ï¼Œã€‚]{2,}', 'ã€‚', content)
        content = content.strip()
        
        # æ£€æŸ¥æ¸…ç†åçš„å†…å®¹è´¨é‡
        # å¦‚æœå¹²æ‰°ä¿¡æ¯å æ¯”è¿‡é«˜ï¼Œè¿”å› None
        remaining_noise = ['å¾®ä¿¡æ‰«ä¸€æ‰«', 'å…³æ³¨è¯¥å…¬ä¼—å·', 'å–æ¶ˆ', 'å…è®¸', 'çŸ¥é“äº†']
        noise_ratio = sum(1 for keyword in remaining_noise if keyword in content) / max(len(content.split()), 1)
        if noise_ratio > 0.1:  # å¦‚æœå¹²æ‰°ä¿¡æ¯å æ¯”è¶…è¿‡10%ï¼Œè§†ä¸ºè´¨é‡å·®
            return None
        
        # å¦‚æœæ¸…ç†åå†…å®¹å¤ªå°‘ï¼Œè¿”å› None
        if len(content) < 50:
            return None
        
        return content
    
    return None

def _fetch_url_content_http(url, is_wechat=False):
    """
    ç­–ç•¥ Aï¼šå¿«é€Ÿ HTTP æŠ“å–
    å‚è€ƒ VC Copilot çš„å®ç°æ–¹å¼
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        }
        
        if is_wechat:
            headers['Referer'] = 'https://mp.weixin.qq.com/'
        
        resp = requests.get(url, headers=headers, timeout=30, allow_redirects=True)
        
        if resp.status_code != 200:
            return False, None
        
        # å…ˆæ£€æŸ¥æ˜¯å¦éœ€è¦éªŒè¯ï¼ˆåœ¨æ¸…ç† HTML å‰æ£€æŸ¥ï¼Œæ›´å¿«ï¼‰
        if 'ç¯å¢ƒå¼‚å¸¸' in resp.text or 'å®ŒæˆéªŒè¯åå³å¯ç»§ç»­è®¿é—®' in resp.text:
            print(f"âš ï¸ æ£€æµ‹åˆ°éªŒè¯é¡µé¢ï¼ŒHTTP æŠ“å–å¤±è´¥")
            return False, None
        
        html = _clean_html_content(resp.text)
        
        # é’ˆå¯¹å¾®ä¿¡å…¬ä¼—å·çš„ç‰¹æ®Šå¤„ç†
        if is_wechat:
            content = _extract_wechat_content(html)
            # æé«˜å†…å®¹è´¨é‡è¦æ±‚ï¼šè‡³å°‘ 200 å­—ç¬¦ï¼Œä¸”ä¸èƒ½å…¨æ˜¯å¹²æ‰°ä¿¡æ¯
            if content and len(content) >= 200:
                # æ£€æŸ¥å†…å®¹è´¨é‡ï¼šå¦‚æœå¹²æ‰°ä¿¡æ¯å æ¯”è¿‡é«˜ï¼Œè§†ä¸ºå¤±è´¥
                noise_keywords = ['å¾®ä¿¡æ‰«ä¸€æ‰«', 'å…³æ³¨è¯¥å…¬ä¼—å·', 'å–æ¶ˆ', 'å…è®¸', 'çŸ¥é“äº†']
                noise_count = sum(1 for keyword in noise_keywords if keyword in content)
                if noise_count > 3:  # å¦‚æœå¹²æ‰°å…³é”®è¯è¶…è¿‡3ä¸ªï¼Œå¯èƒ½å†…å®¹è´¨é‡å·®
                    print(f"âš ï¸ æå–çš„å†…å®¹è´¨é‡ä¸è¶³ï¼ˆå¹²æ‰°ä¿¡æ¯è¿‡å¤šï¼‰ï¼Œå°è¯•å…¶ä»–æ–¹æ³•...")
                    return False, None
                return True, content[:5000]
            elif content and len(content) < 200:
                print(f"âš ï¸ æå–çš„å†…å®¹é•¿åº¦ä¸è¶³ï¼ˆ{len(content)} å­—ç¬¦ï¼‰ï¼Œå¯èƒ½åŒ…å«å¹²æ‰°ä¿¡æ¯")
                return False, None
        
        # æ™®é€šç½‘é¡µï¼šæå–æ­£æ–‡å†…å®¹
        soup = BeautifulSoup(html, 'html.parser')
        text_content = soup.get_text(separator='\n', strip=True)
        
        if len(text_content) > 100:
            return True, text_content[:5000]
        
        return False, None
        
    except requests.exceptions.Timeout:
        return False, None
    except Exception as e:
        print(f"âš ï¸ HTTP æŠ“å–å¤±è´¥: {e}")
        return False, None

def _fetch_wechat_with_playwright(url):
    """
    ç­–ç•¥ Bï¼šPlaywright æµè§ˆå™¨è‡ªåŠ¨åŒ–æŠ“å–
    å‚è€ƒ VC Copilot çš„å®ç°æ–¹å¼
    ä»…åœ¨ HTTP æŠ“å–å¤±è´¥æ—¶ä½¿ç”¨
    """
    if not PLAYWRIGHT_AVAILABLE:
        return False, None
    
    try:
        with sync_playwright() as p:
            # å¯åŠ¨æ— å¤´æµè§ˆå™¨
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080}
            )
            page = context.new_page()
            
            # è®¿é—®é¡µé¢
            page.goto(url, wait_until='domcontentloaded', timeout=30000)
            
            # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½ï¼ˆå¾®ä¿¡å…¬ä¼—å·å†…å®¹å¯èƒ½éœ€è¦ JavaScript åŠ¨æ€åŠ è½½ï¼‰
            import time
            time.sleep(3)  # ç­‰å¾… 3 ç§’è®© JavaScript æ‰§è¡Œ
            
            # å°è¯•æ»šåŠ¨é¡µé¢ä»¥è§¦å‘æ‡’åŠ è½½
            page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            time.sleep(2)
            page.evaluate("window.scrollTo(0, 0)")
            time.sleep(1)
            
            # ç­‰å¾…å†…å®¹åŠ è½½ï¼ˆä½¿ç”¨æ›´å®½æ¾çš„æ¡ä»¶ï¼‰
            try:
                # ç­‰å¾…é¡µé¢æ ‡é¢˜æˆ–ä»»ä½•å†…å®¹å…ƒç´ 
                page.wait_for_load_state('networkidle', timeout=10000)
            except:
                pass  # å¦‚æœè¶…æ—¶ï¼Œç»§ç»­å¤„ç†
            
            # å°è¯•å¤šç§æ–¹å¼è·å–å†…å®¹
            content = None
            
            # æ–¹æ³• 1: å°è¯•è·å– #js_content
            try:
                js_content_elem = page.query_selector('#js_content')
                if js_content_elem:
                    content = js_content_elem.inner_text()
                    if content and len(content) > 100:
                        print(f"âœ… Playwright é€šè¿‡ #js_content è·å–å†…å®¹ {len(content)} å­—ç¬¦")
                        browser.close()
                        return True, content[:5000]
            except:
                pass
            
            # æ–¹æ³• 2: å°è¯•è·å– .rich_media_content
            try:
                rich_content_elem = page.query_selector('.rich_media_content')
                if rich_content_elem:
                    content = rich_content_elem.inner_text()
                    if content and len(content) > 100:
                        print(f"âœ… Playwright é€šè¿‡ .rich_media_content è·å–å†…å®¹ {len(content)} å­—ç¬¦")
                        browser.close()
                        return True, content[:5000]
            except:
                pass
            
            # æ–¹æ³• 3: ä»æ•´ä¸ªé¡µé¢ HTML ä¸­æå–
            html = page.content()
            browser.close()
            
            # æå–å†…å®¹
            html = _clean_html_content(html)
            content = _extract_wechat_content(html)
            
            if content and len(content) >= 200:  # æé«˜è´¨é‡è¦æ±‚
                # æ£€æŸ¥å†…å®¹è´¨é‡
                noise_keywords = ['å¾®ä¿¡æ‰«ä¸€æ‰«', 'å…³æ³¨è¯¥å…¬ä¼—å·', 'å–æ¶ˆ', 'å…è®¸', 'çŸ¥é“äº†']
                noise_count = sum(1 for keyword in noise_keywords if keyword in content)
                if noise_count <= 3:  # å¹²æ‰°ä¿¡æ¯ä¸å¤š
                    print(f"âœ… Playwright é€šè¿‡ HTML è§£æè·å–å†…å®¹ {len(content)} å­—ç¬¦")
                    return True, content[:5000]
            
            return False, None
            
    except Exception as e:
        print(f"âš ï¸ Playwright æŠ“å–å¤±è´¥: {e}")
        return False, None

def extract_content_from_url(url):
    """
    æŠ“å–ç½‘é¡µ/å…¬ä¼—å·æ­£æ–‡
    å‚è€ƒ VC Copilot çš„åŒç­–ç•¥æŠ“å–æœºåˆ¶ï¼š
    1. å…ˆå°è¯•å¿«é€Ÿ HTTP æŠ“å–
    2. å¦‚æœå¤±è´¥æˆ–å†…å®¹ä¸è¶³ï¼Œä½¿ç”¨ Playwrightï¼ˆä»…å¾®ä¿¡å…¬ä¼—å·ï¼‰
    """
    print(f"ğŸŒ æ­£åœ¨æŠ“å–é“¾æ¥å†…å®¹: {url}...")
    
    is_wechat = _is_wechat_url(url)
    
    # å°è¯•æ–¹æ³• 1: Jina Reader APIï¼ˆæœ€å¯é ï¼Œæ”¯æŒå¾®ä¿¡å…¬ä¼—å·ï¼‰
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        jina_url = f"https://r.jina.ai/{url}"
        resp = requests.get(jina_url, headers=headers, timeout=60)
        if resp.status_code == 200 and len(resp.text) > 100:
            # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯é¡µé¢
            if 'ç¯å¢ƒå¼‚å¸¸' not in resp.text and 'å®ŒæˆéªŒè¯åå³å¯ç»§ç»­è®¿é—®' not in resp.text:
                content = resp.text[:5000]
                print(f"âœ… Jina Reader æˆåŠŸæŠ“å– {len(content)} å­—ç¬¦")
                return content
    except requests.exceptions.Timeout:
        print(f"âš ï¸ Jina Reader è¶…æ—¶ï¼ˆ60ç§’ï¼‰ï¼Œå°è¯• HTTP æŠ“å–...")
    except Exception as e:
        print(f"âš ï¸ Jina Reader å¤±è´¥: {e}ï¼Œå°è¯• HTTP æŠ“å–...")
    
    # ç­–ç•¥ Aï¼šå¿«é€Ÿ HTTP æŠ“å–
    print(f"âš¡ å°è¯•å¿«é€Ÿ HTTP æŠ“å–...")
    success, content = _fetch_url_content_http(url, is_wechat=is_wechat)
    
    if success and content and len(content) >= 200:
        print(f"âœ… HTTP æŠ“å–æˆåŠŸ {len(content)} å­—ç¬¦")
        return content
    
    # ç­–ç•¥ Bï¼šPlaywright æµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼ˆä»…å¾®ä¿¡å…¬ä¼—å·ä¸” HTTP å¤±è´¥æ—¶ï¼‰
    if is_wechat and (not success or not content or len(content) < 200):
        if PLAYWRIGHT_AVAILABLE:
            print(f"ğŸ­ HTTP æŠ“å–å¤±è´¥æˆ–å†…å®¹ä¸è¶³ï¼Œå°è¯• Playwright æµè§ˆå™¨æŠ“å–...")
            success, content = _fetch_wechat_with_playwright(url)
            
            if success and content:
                print(f"âœ… Playwright æŠ“å–æˆåŠŸ {len(content)} å­—ç¬¦")
                return content
        else:
            print(f"ğŸ’¡ Playwright æœªå®‰è£…ï¼Œæ— æ³•ä½¿ç”¨æµè§ˆå™¨è‡ªåŠ¨åŒ–æŠ“å–")
    
    print(f"âŒ æ— æ³•æŠ“å–è¯¥é“¾æ¥å†…å®¹")
    if is_wechat:
        print(f"ğŸ’¡ å»ºè®®ï¼šå¯¹äºéœ€è¦éªŒè¯çš„å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œè¯·æ‰‹åŠ¨å¤åˆ¶æ–‡ç« å†…å®¹ä½œä¸ºæ–‡æœ¬è¾“å…¥")
    return None

def extract_text_from_image(image_path):
    """ä½¿ç”¨ OCR ä»å›¾ç‰‡ä¸­æå–æ–‡å­—ï¼ˆå¤‡é€‰æ–¹æ¡ˆï¼Œå› ä¸º DeepSeek ä¸æ”¯æŒå›¾ç‰‡è¾“å…¥ï¼‰"""
    if not OCR_AVAILABLE:
        print("âš ï¸ OCR åŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·å®‰è£… pytesseract å’Œ Pillow")
        return None
    
    try:
        print(f"ğŸ” ä½¿ç”¨ OCR æå–å›¾ç‰‡æ–‡å­—...")
        image = Image.open(image_path)
        # ä½¿ç”¨ä¸­æ–‡å’Œè‹±æ–‡è¯†åˆ«
        text = pytesseract.image_to_string(image, lang='chi_sim+eng')
        if text and len(text.strip()) > 10:
            print(f"âœ… OCR æå–æˆåŠŸï¼Œå…± {len(text)} å­—ç¬¦")
            return text.strip()
        else:
            print("âš ï¸ OCR æœªèƒ½æå–åˆ°æœ‰æ•ˆæ–‡å­—")
            return None
    except Exception as e:
        print(f"âŒ OCR æå–å¤±è´¥: {e}")
        return None

def process_and_save(input_content, input_type="text"):
    """
    æ ¸å¿ƒæµç¨‹ï¼šè¾“å…¥ -> AI è§£æ -> å­˜å…¥æ•°æ®åº“
    """
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    is_image_input = False  # æ ‡è®°æ˜¯å¦ä¸ºå›¾ç‰‡è¾“å…¥
    
    # --- 1. é¢„å¤„ç†è¾“å…¥ ---
    if input_type == "link":
        content = extract_content_from_url(input_content)
        if not content: return
        
        messages.append({"role": "user", "content": f"ç½‘é¡µå†…å®¹ï¼š\n{content}"})
    
    elif input_type == "image_url":
        # DeepSeek ä¸æ”¯æŒå›¾ç‰‡è¾“å…¥ï¼Œä½¿ç”¨ OCR æå–æ–‡å­—åä½œä¸ºæ–‡æœ¬å¤„ç†
        is_image_input = True  # æ ‡è®°ä¸ºå›¾ç‰‡è¾“å…¥
        if os.path.exists(input_content):
            # æœ¬åœ°æ–‡ä»¶ï¼šä½¿ç”¨ OCR æå–æ–‡å­—
            print(f"ğŸ“· è¯»å–æœ¬åœ°å›¾ç‰‡æ–‡ä»¶: {input_content}")
            text_content = extract_text_from_image(input_content)
            if text_content:
                messages.append({"role": "user", "content": f"æµ·æŠ¥å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹ï¼š\n{text_content}\n\nè¯·ä»ä»¥ä¸Šæ–‡å­—ä¸­æå–æ´»åŠ¨ä¿¡æ¯ï¼š"})
            else:
                print("âŒ æ— æ³•ä»å›¾ç‰‡ä¸­æå–æ–‡å­—ï¼Œè¯·æ‰‹åŠ¨è¾“å…¥å›¾ç‰‡å†…å®¹")
                return
        else:
            # URLï¼šå°è¯•ä¸‹è½½åä½¿ç”¨ OCR
            print(f"ğŸ“· ä¸‹è½½å›¾ç‰‡: {input_content}")
            try:
                resp = requests.get(input_content, timeout=15)
                if resp.status_code == 200:
                    # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
                    temp_path = "/tmp/temp_image.jpg"
                    with open(temp_path, 'wb') as f:
                        f.write(resp.content)
                    text_content = extract_text_from_image(temp_path)
                    if text_content:
                        messages.append({"role": "user", "content": f"æµ·æŠ¥å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹ï¼š\n{text_content}\n\nè¯·ä»ä»¥ä¸Šæ–‡å­—ä¸­æå–æ´»åŠ¨ä¿¡æ¯ï¼š"})
                    else:
                        print("âŒ æ— æ³•ä»å›¾ç‰‡ä¸­æå–æ–‡å­—")
                        return
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    os.remove(temp_path)
                else:
                    print(f"âŒ ä¸‹è½½å›¾ç‰‡å¤±è´¥: {resp.status_code}")
                    return
            except Exception as e:
                print(f"âŒ å¤„ç†å›¾ç‰‡ URL å¤±è´¥: {e}")
                return
    
    else: # text
        messages.append({"role": "user", "content": f"ç¾¤æ¶ˆæ¯ï¼š\n{input_content}"})
    
    # --- 2. è°ƒç”¨ AI ---
    print("ğŸ¤– AI æ­£åœ¨è§£æ...")
    try:
        response = openai_client.chat.completions.create(
            model="deepseek-chat", # DeepSeek æ¨¡å‹ï¼Œæ”¯æŒä¸­æ–‡ç†è§£å’Œ JSON è¾“å‡º
            messages=messages,
            response_format={"type": "json_object"},
            temperature=0.1
        )
        result_json = json.loads(response.choices[0].message.content)
    except Exception as e:
        print(f"âŒ AI è§£æå‡ºé”™: {e}")
        return
    
    if not result_json.get("is_valid", True):
        print("âš ï¸ å†…å®¹è¢«åˆ¤å®šä¸ºæ— æ•ˆä¿¡æ¯ï¼Œè·³è¿‡å­˜å‚¨ã€‚")
        return
    
    print(f"âœ… è§£ææˆåŠŸ: {result_json['title']}")
    
    # --- 3. æ£€æŸ¥é‡å¤æ•°æ®ï¼ˆä½¿ç”¨æ™ºèƒ½å»é‡ï¼‰ ---
    title = result_json.get("title")
    event_type = result_json.get("type")
    source_group = result_json.get("source_group", "AI é‡‡é›†")
    
    print("ğŸ” æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ•°æ®ï¼ˆæ™ºèƒ½å»é‡ï¼‰...")
    is_duplicate, existing_id = check_duplicate(title, event_type, source_group)
    
    if is_duplicate:
        print(f"âš ï¸ å‘ç°é‡å¤æ•°æ®ï¼šå·²å­˜åœ¨ç›¸ä¼¼çš„æ´»åŠ¨ï¼ˆID: {existing_id}ï¼‰")
        print(f"   å½“å‰æ ‡é¢˜: {title}")
        print(f"   æ ‡å‡†åŒ–å: {normalize_title(title)}")
        print(f"   ç±»å‹: {event_type}")
        print("   ğŸ’¡ è·³è¿‡æ’å…¥ï¼Œé¿å…é‡å¤æ•°æ®")
        return
    
    # --- 4. å­˜å…¥ Supabase ---
    print("ğŸ’¾ æ­£åœ¨å†™å…¥æ•°æ®åº“...")
    try:
        # æ„é€ è¦å†™å…¥çš„æ•°æ® (åŒ¹é…æ•°æ®åº“å­—æ®µ)
        
        # å¤„ç† raw_contentï¼šå¦‚æœæ˜¯å›¾ç‰‡è¾“å…¥ï¼Œä¸å­˜å‚¨æœ¬åœ°è·¯å¾„ï¼Œè€Œæ˜¯å­˜å‚¨æ ‡è¯†
        if is_image_input:
            # å›¾ç‰‡è¾“å…¥ï¼šå­˜å‚¨æ ‡è¯†ä¿¡æ¯ï¼Œè€Œä¸æ˜¯æœ¬åœ°æ–‡ä»¶è·¯å¾„
            raw_content = "ğŸ“· å›¾ç‰‡æµ·æŠ¥ï¼ˆå·²é€šè¿‡ OCR æå–ä¿¡æ¯ï¼‰"
        else:
            # æ–‡æœ¬æˆ–é“¾æ¥è¾“å…¥ï¼šå­˜å‚¨åŸå§‹å†…å®¹ï¼ˆå‰500å­—ï¼‰
            raw_content = input_content[:500] if isinstance(input_content, str) else str(input_content)[:500]
        
        db_data = {
            "title": title,
            "type": event_type,
            "source_group": result_json.get("source_group", "AI é‡‡é›†"),
            "publish_time": "åˆšåˆš",  # å¿…éœ€å­—æ®µï¼ŒAI é‡‡é›†çš„æ•°æ®æ ‡è®°ä¸º"åˆšåˆš"
            "key_info": result_json.get("key_info", {}), # JSONB ç›´æ¥å­˜
            "summary": result_json.get("summary"),
            "tags": result_json.get("tags", []),
            "raw_content": raw_content, # æ ¹æ®è¾“å…¥ç±»å‹å¤„ç†
            "status": "active"
        }
        
        data, count = supabase.table("events").insert(db_data).execute()
        print("ğŸ‰ æˆåŠŸå…¥åº“ï¼å°ç¨‹åºåˆ·æ–°å¯è§ã€‚")
        
    except Exception as e:
        error_msg = str(e)
        if "row-level security policy" in error_msg.lower():
            print(f"âŒ æ•°æ®åº“å†™å…¥å¤±è´¥: RLS ç­–ç•¥é˜»æ­¢äº†æ’å…¥æ“ä½œ")
            print("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼šè¯·åœ¨ Supabase æ§åˆ¶å°æ‰§è¡Œä»¥ä¸‹ SQL æ¥å…è®¸æ’å…¥ï¼š")
            print("""
CREATE POLICY "Allow service role to insert events"
    ON events
    FOR INSERT
    TO service_role
    WITH CHECK (true);
            """)
            print("æˆ–è€…ä½¿ç”¨ service_role key è€Œä¸æ˜¯ anon keyï¼ˆæ›´å®‰å…¨ï¼‰")
        else:
            print(f"âŒ æ•°æ®åº“å†™å…¥å¤±è´¥: {e}")

# --- ğŸš€ è¿è¡Œå…¥å£ ---
if __name__ == "__main__":
    
    # åœºæ™¯ 1: æ¨¡æ‹Ÿä¸€æ®µç¾¤é‡Œçš„æ‹›è˜æ¶ˆæ¯
    print("\n--- ä»»åŠ¡ 1: å¤„ç†æ–‡æœ¬ ---")
    raw_text = """
ç¾å›¢-å•†ä¸šåˆ†æå®ä¹ ç”Ÿ-å•†ä¸šåŒ–æˆ˜ç•¥æ–¹å‘ï¼ˆbaseåŒ—äº¬ï¼‰

ä¸€ã€å²—ä½èŒè´£ï¼š

åœ¨å¯¼å¸ˆçš„æŒ‡å¯¼ä¸‹é€æ­¥ç‹¬ç«‹æ‰¿æ¥ä»¥ä¸‹é‡ç‚¹å·¥ä½œï¼š

1ã€äº§ä¸šç ”ç©¶ï¼šèšç„¦æœ¬åœ°ç”Ÿæ´»é¢†åŸŸçš„å¤–å–åŠåˆ°åº—è¡Œä¸šï¼Œå¼€å±•å•†ä¸šåŒ–æ¨¡å¼ç ”ç©¶ï¼›çœ‹æ¸…äº§ä¸šé“¾ï¼Œæ´å¯Ÿè¡Œä¸šè¶‹åŠ¿ï¼Œè¯†åˆ«å¢é•¿æœºä¼šï¼Œæ”¯æ’‘é•¿æœŸä¸šåŠ¡æˆ˜ç•¥è§„åˆ’ï¼›

2ã€ç«äº‰åˆ†æï¼šå¼€å±•ç³»ç»Ÿæ€§ç«äº‰ç›‘æ§å’Œä¸“é¢˜ç ”ç©¶ï¼Œè¯†åˆ«é£é™©å’Œæœºä¼šï¼›

3ã€å•†ä¸šåˆ†æï¼šå›´ç»•å•†ä¸šåŒ–å˜ç°ï¼Œå¼€å±•æ ‡æ†æ¨¡å¼æˆ–è€…ç›¸å…³ç»è¥è¯¾é¢˜å¼€å±•æ·±å…¥åˆ†æï¼Œè§£ç­”ä¸šåŠ¡å‘å±•é—®é¢˜ï¼ŒååŒä¸šåŠ¡è½åœ°ã€‚

äºŒã€å²—ä½è¦æ±‚ï¼š

1ã€å­¦å†èƒŒæ™¯ä¼˜ç§€ï¼Œå…·å¤‡å‡ºè‰²çš„é€»è¾‘æ€ç»´èƒ½åŠ›å’Œå¿«é€Ÿçš„å­¦ä¹ èƒ½åŠ›ï¼Œä¹äºæ€è€ƒï¼›

2ã€å¯¹äºå•†ä¸šåŒ–ã€æˆ˜ç•¥ã€è¡Œä¸šç ”ç©¶å……æ»¡å¥½å¥‡å¿ƒï¼Œå¹¶ä»¥æ­¤ä¸ºé•¿æœŸèŒä¸šå‘å±•æ–¹å‘ï¼›

3ã€æ¯å‘¨å®ä¹ 4-5å¤©ï¼Œè¿ç»­å®ä¹ 6ä¸ªæœˆä»¥ä¸Šï¼ˆå¿…é¡»ï¼‰ï¼Œæœ‰è½¬æ­£æœºä¼šï¼›

4ã€ä¼˜å…ˆå¤§å››ä¿ç ”ã€å…¨æ—¥åˆ¶MBAåŒå­¦ã€‚

ä¸‰ã€å²—ä½äº®ç‚¹ï¼š

1ã€ç¾å›¢å¤–å–åŠåˆ°åº—ä¸šåŠ¡æœªæ¥æ”¶å…¥å¢é•¿ç‚¹ï¼Œèšç„¦æ–°ä¸šåŠ¡æ¨¡å¼çš„ç ”ç©¶å’Œæ¢ç´¢ï¼›

2ã€å›¢é˜Ÿæ‹¥æœ‰æˆç†Ÿå®Œå–„çš„æˆ˜ç•¥è¡Œç ”æ–°äººåŸ¹å…»æ–¹æ³•å’Œä½“ç³»ï¼Œé€‚åˆå¸Œæœ›èŒä¸šè½¬å‹æˆ–è¿›å…¥æˆ˜ç•¥ã€å•†ä¸šåˆ†æé¢†åŸŸçš„åŒå­¦ã€‚

å››ã€æŠ•é€’ä¿¡æ¯ï¼š

è¯·å°†ç®€å†é‚®ä»¶å‘é€è‡³ï¼šproj.ba.recruit@meituan.comï¼›é‚®ä»¶æ ‡é¢˜åŠç®€å†å‘½åï¼šã€å®ä¹ ã€‘å§“å+å­¦æ ¡å¹´çº§+ä¸“ä¸š+æœ€å¿«åˆ°å²—æ—¶é—´+å¯å®ä¹ æ—¶é•¿+ç”µè¯
    """
    
    process_and_save(raw_text, "text")
    
    # åœºæ™¯ 2: æµ‹è¯•é“¾æ¥è§£æ
    print("\n--- ä»»åŠ¡ 2: å¤„ç†é“¾æ¥ ---")
    test_url = "https://mp.weixin.qq.com/s/sT5-QQ9jNXxi7VWv_M0HAw?scene=1&click_id=6"
    process_and_save(test_url, "link")
    
    # åœºæ™¯ 3: æµ‹è¯•å›¾ç‰‡è§£æï¼ˆæœ¬åœ°æ–‡ä»¶ï¼‰
    print("\n--- ä»»åŠ¡ 3: å¤„ç†å›¾ç‰‡ ---")
    # ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„å›¾ç‰‡æ–‡ä»¶
    image_path = pathlib.Path(__file__).parent.parent / "å¾®ä¿¡å›¾ç‰‡_20251201135625_529_1500.jpg"
    if image_path.exists():
        process_and_save(str(image_path), "image_url")
    else:
        print(f"âš ï¸ å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
        print("ğŸ’¡ è¯·å°†å›¾ç‰‡æ–‡ä»¶æ”¾åœ¨é¡¹ç›®æ ¹ç›®å½•ï¼Œæˆ–ä¿®æ”¹ image_path å˜é‡")
